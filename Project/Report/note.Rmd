---
title: "Note"
author: "Riccardo Ferrarese"
date: "19/3/2021"
output: pdf_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60), tidy=TRUE, 
                      python.reticulate=FALSE, 
                      echo = TRUE, 
                      warning = FALSE, 
                      cache = TRUE, 
                      message = FALSE, 
                      out.width = "80%"
                      )
```

#### questions todo
- Who are the most active users (in terms of number of posts and length of the messages)? 

- How did these emotions change over time? 


# Analysis of topics about cryptocurrencies on Reddit

## Setup App

Le librerie utilizzate per eseguire l'analisi dei dati estratti da reddit sono le seguenti: 

```{r library, result='hide'}

# general lib for manipulate data
library(dplyr)
library(tidyr)
library(tidyverse)

# lib for side by side plot 
library(patchwork)

# lib for network analysis
library(ggraph)
library(tidygraph)
library(lubridate)

# lib for text mining
library(tidytext)
library(quanteda)

# lib for sentiment dataset
library(textdata)

# lib for lemmatize words
library(textstem)

# lib for stemming words
library(hunspell)
library(SnowballC)

```

Per la gestione del progetto sono state utilizzati anche i seguenti pacchetti: 

- installare [*git*](http://try.github.io/), sistema per il controllo del versionamento 
- installare [*renv*](https://blog.rstudio.com/2019/11/06/renv-project-environments-for-r/), permette di creare un ambiente di esecuzione isolato per la gestione delle dipendenze. Permette anche di utilizzare il pacchetto _reticulate_ per integrare degli script in Python e gestire l'ambiente locale per la loro esecuzione.
- *shiny app* per creare l'applicazione shiny per visualizzare i risultati

## Reddit 

Reddit, a differenza della maggior parte dei social-network maggiormente utilizzati, non presenta un meccanismo per scegliere una cerchia di utenti con cui interagire (per intenderci,  i followers su Twitter o Instagram), tuttavia il modo in cui è organizzato permette all'utente di interagire in diversi forums, ognuno di uno specifico argomento, nel quale interagirà con gli altri utenti che 'seguo' lo stesso _subreddit_. Inoltre sulla pagina generale si possono trovare i post più popolari di tutta la piattaforma. 

Si può definire Reddit come un _social-network aggregator_, cioè è una piattaforma di discussione in cui gli utenti possono discutere o condividere informazioni, suddivisa in forums di specifici argomenti chiamati **subreddits**. 

Ogni utente può eseguire un numero illimitato di interazioni con la piattaforma, dove le interazioni possono essere la scrittura di un post in uno specifico subreddit, commentare post e interazioni di altri utenti o esprimere la propria preferenza relativa a un certo post o commento. Se un certo post (a meno che non sia di un subreddit privato)  riceve molti _downvotes_ immediatamente crollerà sulla 'classifica dei post' e scoparirà dalla vista degli altri utenti. Al contrario se acquisisce una certa importanza potrà esser visualizzato nella pagina generale di reddit e raggiungendo così un numero maggiore di utenti. 

Un ulteriore particolarità di questa piattaforma è che gli utenti sono allo stesso modo creatore di contenuti, consumatori e curatori delle informazioni in esso. Utilizza infatti un sistema a punteggio, tramite _upvotes_ e _downvotes_ da parte degli utenti, per determinare i contenuti e le discussioni con maggior interesse e che verrano mostrati nei primi contenuti della pagina. 

Pur permettendo agli utenti di mantenere l'anonimicità utilizzando un nome utente a piacere, Reddit tiene traccia di tutte le attività di ogni profilo inclusi i post e i commenti effettuati. 


## Collect Data with Python

Per la raccolta dati è stato utilizzato Python dal momento che è presente la libreria _Pushshift_ per l'estrazione dei dati di Reddit senza vincoli stringenti sulle richieste effettuate. 
E' stato utilizzato un wrapper delle Python Reddit API, **pmaw**, che permette di eseguire lo scrapping dei dati utilizzando il multithreading, in modo tale da rendere più efficente il processo di raccolta dei dati.

Le API mettono a disposizione una serie di funzioni per la ricerca dei post d'interesse, potendo specificare l'intervallo temporale e i subreddit in cui eseguire la ricerca. Inoltre, volendo estrarre da Reddit le informazioni relative a diversi subreddit è stata utilizzata la libreria _multiprocessing_ per lanciare un _pool_ di processi incaricati di eseguire la stessa funzione su dati diversi. 

```{python main, eval=FALSE}
LIST_of_SUBREDDIT = [ 'dogecoin',  
    'pancakeswap', 
    'eth',
    'ethereum', 
    'elon', 
    'wallstreetbets']

    start_epoch=int(dt.datetime(2020, 1, 1).timestamp())
    end_epoch=int(dt.datetime(2021, 2, 1).timestamp())

    nargs = []
    for x in LIST_of_SUBREDDIT:
        nargs.append( [start_epoch, end_epoch, 'submissions', x] )
        nargs.append( [start_epoch, end_epoch, 'comments', x]  )

    pool = Pool(16)
    pool.map(run, nargs)
    pool.close()
    pool.join()

```

La funzione _run_ lanciata da _pool.map_ istanzia un oggetto di classe Miner contente la chiave di autenticazione per accedere a Pushshift e i metadati delle informazioni da raccogliere. In questo modo vengono lanciati diversi **Miner**, ognuno per uno specifico subreddit, i quali eseguono la ricerca e il salvataggio dei dati in parallelo. 

```{python func-class, eval=FALSE}
def run(args) -> pd.DataFrame: 
    """
    Function for starts miner's' process

    Args: 
        [start, end]: [temporal intervall where we would scrap data]
        [item]: element to scrap
    Returns:
        [type]: [description]
    """

    print(args)
    start, end, item, subreddit = args

    miner = Miner(start, end, item, subreddit)
    miner.perform_search()
    return miner.read_data()

class Miner(object):
    """ Class for Reddit Data Mining"""
    
    def __init__(self, start_epoch, end_epoch, func, subreddit) -> None:
        super().__init__()
        self.api = PushshiftAPI(rate_limit=100) 
        self.start_time = start_epoch
        self.end_time= end_epoch
        self.subreddit = subreddit
        self.data = None 
        self.func = func

    def read_data(self): 
        return self.data

    def perform_search(self):
        item = self.func 
        print(f'Start search {item}...')
        if item == 'submissions': 
            df = self.search_save_sub(self.subreddit)
            self.data = df
        if item == 'comments':
            df = self.search_save_com(self.subreddit)
            self.data = df
    
    @timeit
    def search_save_sub(self, subreddit): 
        api = self.api
        res_ = api.search_submissions(after=self.start_time,
                                before=self.end_time, 
                                subreddit=subreddit,
                                filter=COLS_SUB, 
                                #limit=2
                                )
        data = pd.DataFrame([x for x in res_])
        data.to_csv(f"./data/{self.subreddit}_sub.csv")
        print(f"write {self.subreddit}_sub.csv")
        
    @timeit
    def search_save_com(self, subreddit):
        api = self.api
        res_ = api = self.api.search_comments(after=self.start_time,
                                before=self.end_time, 
                                subreddit=subreddit,
                                filter=COLS_COM, 
                                #limit=2
                            )
        data = pd.DataFrame([x for x in res_])
        data.to_csv(f"./data/{self.subreddit}_com.csv")
        print(f"write {self.subreddit}_com.csv")
    
```

E' stato utilizzato un _rate-limit_ per le richieste all'API pari a 100, leggermente superiore al limite di default ma leggermente inferiore al limite imposto dalle richieste al minuto massime che si possono effettuare per la raccolta. 

La funzione _decorator timeit_ permette inoltre di avere una misura indicativa del tempo impiegato per ogni richiesta. In totale l'esecuzione ha impiegato ..., ma essendo eseguita in parallelo il tempo reale di esecuzione è stato di un paio di ore. 

Di seguito è mostrato un estratto di output da terminale che si ottiene dopo l'esecuzione del programma. 

```{shell eval=FALSE}
args first process: [1577833200, 1614553200, 'submissions']
args second process: [1577833200, 1614553200, 'comments']
Start search comments...
Start search submissions...
1430174 results available in Pushshift
259899 results available in Pushshift
Checkpoint:: Success Rate: 36.00% - Requests: 100 - Batches: 10 - Items Remaining: 256299
Checkpoint:: Success Rate: 29.00% - Requests: 100 - Batches: 10 - Items Remaining: 1427274
Checkpoint:: Success Rate: 35.50% - Requests: 200 - Batches: 20 - Items Remaining: 253225
Checkpoint:: Success Rate: 27.50% - Requests: 200 - Batches: 20 - Items Remaining: 1424674
Checkpoint:: Success Rate: 33.00% - Requests: 300 - Batches: 30 - Items Remaining: 250824
Checkpoint:: Success Rate: 29.33% - Requests: 300 - Batches: 30 - Items Remaining: 1421374
Checkpoint:: Success Rate: 31.00% - Requests: 400 - Batches: 40 - Items Remaining: 248435
Checkpoint:: Success Rate: 30.75% - Requests: 400 - Batches: 40 - Items Remaining: 1417879

[...]

write ./doge_sub.csv
time: 194798997.85995483 ~~ 3,25h 

```


## Shiny App
A reactive expression is an R expression that uses widget input and returns a value. The reactive expression will update this value whenever the original widget changes.



## Overview of Datasets


## Text Analysis 

### Quanteda Library 
**Quanteda** è pacchetto per R per manipolare e analizzare dati testuali in formato non-tidy, sviluppata da Kenneth Benoi, Kohei Watanabe e altri contributors. Definisce una serie di funzioni per applicare processi di NLP (Natural Language Processing) a partire da testi di qualsiasi tipo o documenti, fino all'analisi finale. 

In questo progetto è stata utilizzata per creare i _corpus_ a partire dai dati estratti da reddit contenenti il testo dei commenti. Per comodità di utilizzo del pacchetto, sono state utilizzate le funzioni per la creazione dei _tokens_ a partire dai commenti non manipolati, utilizzando le funzioni built-in per la rimozione di punteggiatura, simboli e le stop-words generali. 

Successivamente alla manipolazione e pulizia dei commenti è stata utilizzata la funzione _dfm_ per creare la _document-feature matrix_, in modo tale da rendere compatibile il tipo del dato con il pacchetto _tidytext_ per la continuare l'analisi. 

Una funzione motlo utile messa a disposizione dal pacchetto è quella per calcolare il valore di tfidf per la matrice dei documenti. Tuttavia, in questo caso di studio, risulta poco utile nel contesto di un singolo subreddit dal momento che si considera come documento ogni singolo commento.

Potrebbe però esser utile per comparare l'insieme totale di commenti di un subreddit con il contenuto presente negli altri. 

### Function for Text Mining

In questa sezione si mostrano le funzione definite per manipolare, pulire e visualizzare i dati estratti da reddit. 

Si è scelto di definire delle funzioni il più generali possibili, in modo tale da rendere più efficiente l'analisi di diversi dataset che condividono le stesse variabili esplicative. Le stesse funzioni possono inoltre esser utilizzate nel server della _shiny app_. 

La prima funzione definita verrà utilizzata per creare il dataset di partenza (chiamato _corpus_) a partire dai dati grezzi salvati in formato csv dallo script in Python. L'algoritmo esegue i seguenti passaggi: 
- converte in un formato comodo per R la colonna rappresentate le date di creazione dei commenti. - aggiunge un id per ogni riga. 
- rimuove i commenti cancellati o quelli che sono stati sottoposti ad una moderazione. 
- crea il corpus utilizzando la funzione messa a disposizione dal pacchetto _quanteda_. 

```{r text_mining}

# data must have cols : (created_utc, author, body )
# data is pushshift's returned csv
create_corpus_from_rawdata <- function(data){
   
   # compute type date
   # add unique id for each comment 
   # filter removed comment
   data_clean <- data %>%
      mutate(date = as.Date(as_datetime(created_utc))) %>%
      mutate(id = row_number()) %>%
      filter(author != '[deleted]' & author != 'AutoModerator') %>% 
      select(id, date, author, body) 
   
   # remove source data for save space
   remove(data)
   
   # quanteda corpus function 
   corpus <- corpus( as.character(data_clean$body), 
                     docnames = data_clean$id, 
                     docvars = data.frame(data_clean$author, data_clean$date))

   return(list("df" = data_clean, "corpus" = corpus))
}
```

Le seguenti tre funzioni hanno lo scopo di calcolare i _token_ per ciascun commento, eseguire una pulizia di essi ed calcolare le ripetizioni di ciascuno. 

La funzione che calcola i token permette di scegliere, tramite argomento, se calcolare i token contenenti una singola parola oppure se calcolare i bigrammi o trigrammi per eseguire un'analisi tenendo conto del contesto tra le diverse parole che si trovano 'vicine' nel commento. 

Successivamente le parole vengono filtrate in modo tale da rimuove caratteri speciali che si trovano agli estremi di esse, infatti reddit permette di inserire dei caratteri speciali per enfatizzare le parole, similmente all' _underscore_ e asterischi nel formato markdown. Inoltre vengono rimosse le parole formate da uno e due caratteri, in modo tale da ridurre il numero di onomatopee che vengono utilizzate nel linguaggio dei messaggi. Questa scrematura non basta per avere le solo parole con un significato compiuto nel dataset, ma dal momento che 'parole' prive di significato avranno poche ripetizioni all'interno di tutto il corpus di testo, potranno esser rimosse successivamente dai grafici con un filtro sul conteggio. 

Dopo una prima pulitura delle parole è possibile, tramite argomento della funzione, specificare se si vuole o meno eseguire lo _stemming_ oppure la _lemmatization_. Nel caso di questa analisi si è scelto di utilizzare lo stemming essendo che la lemmatizzazione potrebbe portare ad alcuni cambiamenti morfologici della parole. La lemmatizzazione potrebbe causare una distorsione di parole che nel campo finanziario acquisiscono un preciso significato oppure rischierebbe di alterare la successiva analisi dei sentimenti del testo. 

Per lo stemming è stata utilizzata la libreria _snowBallC_ utilizzando la lingua inglese, essendo che l'algoritmo predefinito della funzione (alg. di Porter) va a modificare le lettera finale _y_ delle parole con il carattere _i_, non permettendo il pattern matching con il dizionario per l'analisi dei sentimenti.


```{r corpus-tidy-count-function}
## tokenize corpus and make document-feature matrix
# data :: text corpus 
# stop :: bool for remove stop word
# ngrams :: {1,2,3} for compute corpus on ngrams
create_tidy_corpus <- function(data, stop=TRUE, ngrams=1){
   
   # if stopword .. 
      # build toks  
   if(stop){
      toks <- quanteda::tokens_select(
                  quanteda::tokens(data,
                         remove_punct = TRUE, 
                         remove_symbols = TRUE, 
                         remove_numbers = TRUE, 
                         remove_url = TRUE),
               pattern = stopwords("en"), selection = "remove")
   } else {
      toks <- quanteda::tokens(data,
                         remove_punct = TRUE, 
                         remove_symbols = TRUE, 
                         remove_numbers = TRUE, 
                         remove_url = TRUE)
   }
   
   # if ngrams -- compute toks for bigrams or trigrams 
   if(ngrams==2){
      dfm <- quanteda::dfm(quanteda::tokens_ngrams(toks, n = 2, concatenator = " "))
   } else if(ngrams==3){
      dfm <- quanteda::dfm(quanteda::tokens_ngrams(toks, n = 3, concatenator = " "))
   } else {
      dfm <- quanteda::dfm(toks)
   }
   
   return(tidy(dfm))
}

## function for clean data and separate ngrams 
# tidy :: dataframe compute by create corpus ( tidy dfm )
# ngrams :: {1,2,3} for compute corpus on ngrams
compute_clean_corpus_tidy <- function(tidy, ngrams=1, stemming=FALSE, lemma=TRUE){
   
   if(stemming & lemma){stop("Please choose only one between stemming vs lemmatization")}
   # check ngrams and build clean dataframe
   if(ngrams == 2){
      clean_df <- tidy %>%
         ## mutate term in word
            unnest_tokens(words, term, token = "ngrams", n=2) %>% 
         ## divide bigrams in words
            tidyr::separate(words, c("word1", "word2"), sep = " " )   %>%       
            filter( word1 != word2 & word2 != word1 ) %>%
         ## extract word without weird char
            mutate(word1 = str_extract(word1,  "[a-z]+"))  %>%                   
            mutate(word2 = str_extract(word2,  "[a-z]+")) %>% 
         ## remove word with 1-2 letter
            mutate(word1 = gsub(" *\\b[[:alpha:]]{1,2}\\b *", '', word1)) %>%
            mutate(word2 = gsub(" *\\b[[:alpha:]]{1,2}\\b *", '', word2)) %>%
            filter(word1 != "", 
                   word2 != "")
      
      ## lemmatize words
      if(lemma){
         clean_df <- clean_df %>%
            mutate(word1 = textstem::lemmatize_words(word1)) %>%
            mutate(word2 = textstem::lemmatize_words(word2))
      ## stemming words
      } else if(stemming){
         clean_df <- clean_df %>%
            mutate(word1 = SnowballC::wordStem(word1, language = "english")) %>%
            mutate(word2 = SnowballC::wordStem(word2, language = "english"))
      }
      
   } else if (ngrams == 3){
      clean_df <- tidy %>%
            unnest_tokens(words, term, token = "ngrams", n=3) %>%                     
            tidyr::separate(words, c("word1", "word2", "word3"), sep = " " ) %>%   
            filter( word1 != word2 & word2 != word3 & word1 != word3 ) %>%
            mutate(word1 = str_extract(word1,  "[a-z]+"))  %>%                         
            mutate(word2 = str_extract(word2,  "[a-z]+")) %>% 
            mutate(word3 = str_extract(word3,  "[a-z]+")) %>% 
            mutate(word1 = gsub(" *\\b[[:alpha:]]{1,2}\\b *", '', word1)) %>%
            mutate(word2 = gsub(" *\\b[[:alpha:]]{1,2}\\b *", '', word2)) %>%
            mutate(word3 = gsub(" *\\b[[:alpha:]]{1,2}\\b *", '', word3)) %>%
            filter(word1 != "", 
                   word2 != "", 
                   word3 != "")
      
            ## lemmatize words
      if(lemma){
         clean_df <- clean_df %>%
            mutate(word1 = textstem::lemmatize_words(word1)) %>%
            mutate(word2 = textstem::lemmatize_words(word2)) %>%
            mutate(word3 = textstem::lemmatize_words(word3)) 
      ## stemming words
      } else if(stemming){
         clean_df <- clean_df %>%
            mutate(word1 = SnowballC::wordStem(word1, language = "english")) %>%
            mutate(word2 = SnowballC::wordStem(word2, language = "english")) %>%
            mutate(word3 = SnowballC::wordStem(word3, language = "english"))
      }
       
   } else {
      clean_df <- tidy %>%
            unnest_tokens(words, term)  %>%                      
            mutate( words = str_extract(words,  "[a-z]+")) %>%   
            mutate( words = gsub(" *\\b[[:alpha:]]{1,2}\\b *", '', words)) %>%     
            #mutate( words = gsub("^(?!\b(.)\1+\b)", '', words)) %>%
            filter( words != "" ) 
            ## lemmatize words
      if(lemma){
         clean_df <- clean_df %>%
            mutate(words = textstem::lemmatize_words(words)) 
      ## stemming words
      } else if(stemming){
         clean_df <- clean_df %>%
            mutate(words = SnowballC::wordStem(words, language = "english"))
                   # with porter buy -> bui 
      }
      
   }
   
   return(clean_df)
}

## function for word (or ngrams) counts and plot 
# data :: clean dataframe compute by compute clean corpus ( tidy dfm )
# threshold_count :: value to filter items by count for pritty plot
# threshold_freq :: value to filter items by frequency for pritty plot
# ngrams :: {1,2,3} for compute corpus on ngrams
# bool_plot_count :: boolean to indicate whether to plot word counts or not 
# bool_plot_freq :: boolean to indicate whether to plot word frequencies or not
compute_plot_count_and_freq <- function(data, threshold_count=1000, threshold_freq=0.001, ngrams=1, 
                                        bool_plot_count=TRUE, bool_plot_frequency=TRUE){

   if(ngrams == 2){
      # merge single word
      words_unite <- data %>%
         unite(words, c("word1", "word2"), sep = " ") 
      # sum count col for each word
      word_counts <- aggregate(cbind(count) ~ words, data=words_unite, FUN=sum) 
      
   } else if(ngrams == 3){
      words_unite <- data %>%
         unite(words, c("word1", "word2", "word3"), sep = " ") 
      word_counts <- aggregate(cbind(count) ~ words, data=words_unite, FUN=sum) 
      
   } else {
      word_counts <- aggregate(cbind(count) ~ words, data=data, FUN=sum)
   }
   
   total_of_word <- sum( word_counts$count )
   word_counts <- word_counts %>%
      mutate( count = as.integer(count)) %>%
      mutate( total_of_word = total_of_word ) %>%
      mutate( frequency = count/total_of_word)
   
   # plot for count col
   plot_freq <- word_counts %>%
      filter( frequency > threshold_freq) %>%
      ggplot(aes(words, frequency)) + 
         geom_point(alpha = 0.3, size = 1.5, width = 0.25, height = 0.1) + 
         geom_text(aes(label = words), check_overlap = TRUE, vjust = 1) + 
         theme_minimal() + 
         theme(axis.text.x=element_blank()) 
   
   # plot for frequency col
   plot_count <- word_counts %>%
      filter( count > threshold_count) %>%   
      ggplot(aes(words, count)) + 
         geom_point(alpha = 0.3, size = 1.5, width = 0.25, height = 0.1) + 
         geom_text(aes(label = words), check_overlap = TRUE, vjust = 1) + 
         scale_y_log10() + 
         theme_minimal() + 
         theme(axis.text.x=element_blank()) 
      
   
   # print selected plot 
   if(bool_plot_count & bool_plot_frequency){
      print(plot_freq + plot_count + plot_layout(ncol=1, heights = c(4,4)) )
   } else if(bool_plot_count){
      print(plot_count)
   } else if(bool_plot_frequency){
      print(plot_freq)
   }
  
    # split words for sentiment analysis 
   if(ngrams == 3 ){
      word_counts <- word_counts %>%
         tidyr::separate(words, c("word1", "word2", "word3"), sep = " " )
   } else if(ngrams == 2){
      word_counts <- word_counts %>%
         tidyr::separate(words, c("word1", "word2"), sep = " " )
   }
   
   return(word_counts)
   
}
```

Le funzioni definite per l'analisi del sentimento del testo permettono di classificare ciascuna parola (o n-gramma) secondo i valori built-in dei dataset messi a disposizione nella libreria _textdata_. 

Per un analisi generale dei sentimenti di ciascun subreddit si è scelto di utilizzare i sentimenti elencati nel dataset _"nrc"_. Per classificare ciascuna parola sono stati utilizzati entrambe le categorizzazioni messe a disposizione da _"afinn"_, che permette di avere un punteggio numerico per identificare la positività o meno del sentimento, e da _"bing"_, che più semplicemente da solo una categorizzazione tra parole positive e negative. 
L'ultima funzione permette di visualizzare la rete che rappresenta le relazioni che intercorrono tra le singole parole contenute nei bigrammi e trigrammi. 

```{r sentiment-function}

## function for print number of word for each sentiment in nrc df 
# data :: word_frequency
# n_filter_sentiment :: value to filter number of sentiment for pritty plot
# ngrams :: {1,2,3} for compute corpus on ngrams
plot_class_sentiment <- function(data, n_filter_sentiment, ngrams=1){
   # inner join with nrc sentiment
   if(ngrams == 3 ){
      sentiment_class <- data %>%
         # join sentiment with each word 
         inner_join(get_sentiments("nrc"), by=c('word1' = 'word')) %>%
         inner_join(get_sentiments("nrc"),by=c('word2' = 'word')) %>%
         inner_join(get_sentiments("nrc"),by=c('word3' = 'word')) %>%
         # merge sentiment
         unite(sentiment, c("sentiment.x", "sentiment.y", "sentiment"), sep="-") %>%
         # group and count 
         group_by(sentiment) %>% 
         summarise(word_4_sentiment = n()) %>% 
         arrange(-word_4_sentiment, sentiment)
      
      # plot number of word for each sentiment class
      plot <- sentiment_class %>% 
         filter(word_4_sentiment > n_filter_sentiment ) %>%
            ggplot(aes(word_4_sentiment, sentiment, fill=sentiment)) + 
            geom_col( show.legend = FALSE) + 
            xlab("") +
            theme_minimal() 
      
            
   } else if(ngrams == 2 ){
      sentiment_class <- data %>%
         inner_join(get_sentiments("nrc"), by=c('word1' = 'word')) %>%
         inner_join(get_sentiments("nrc"),by=c('word2' = 'word')) %>%
         unite(sentiment, c("sentiment.x", "sentiment.y"), sep="-") %>%
         group_by(sentiment) %>% 
         summarise(word_4_sentiment = n()) %>% 
         arrange(-word_4_sentiment, sentiment) 
      
         
      plot <- sentiment_class %>% 
               filter(word_4_sentiment > n_filter_sentiment ) %>%
                  ggplot(aes(word_4_sentiment, sentiment, fill=sentiment)) + 
                  geom_col( show.legend = FALSE) + 
                  xlab("") +
                  theme_minimal() 
      
      
   } else {
      sentiment_class <- data %>%
         inner_join(get_sentiments("nrc"), by=c('words'= 'word')) %>%
         group_by(sentiment) %>%
         summarise(word_4_sentiment = n()) %>% 
         arrange(-word_4_sentiment, sentiment) 
      
      plot <- sentiment_class %>% 
               #filter(word_4_sentiment > 2500 ) %>%
                  ggplot(aes(word_4_sentiment, sentiment, fill=sentiment)) + 
                  geom_col( show.legend = FALSE) + 
                  xlab("") +
                  theme_minimal() 
   }
   
   
   print(plot)
   return(sentiment_class)
}

# function for normalize value between (0-1]
# usefull for afinn sentiment who have value [-4, 4]
range01 <- function(x){
   min <- min({{x}})
   #print(min)
   max <- max({{x}})
   #print(max)
   (x-min+1)/(max-min+1)
}


## function for print words splited by sentiment
# data :: word_frequency 
# n_filter :: value to filter words by count for pritty plot
# ngrams :: {1,2,3} for compute corpus on ngrams
plot_word_per_sentiment <- function(data, n_filter=20, ngrams=1){
   
   if(ngrams == 2){
      ## with affin --> mean between value.x & value.y
      ## with bing 
         ## positive - positive --> positive
         ## positive - negative --> neutral
         ## negative - positive --> neutral
         ## negative - negative --> negative
      
      sentiment_df <- data %>% 
            inner_join(get_sentiments("afinn"), by=c('word1'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word1'= 'word')) %>%
            inner_join(get_sentiments("afinn"), by=c('word2'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word2'= 'word')) %>%
            mutate( affin = (value.x + value.y) / 2) %>%
            mutate( bing = case_when(sentiment.x == 'positive' & 
                                        sentiment.y == 'positive' ~ 'positive', 
                                     sentiment.x == 'negative' & 
                                        sentiment.y == 'negative' ~ 'negative',
                                     TRUE ~ 'neutral')) %>%
            unite(words, c("word1", "word2"), sep = " ") 
         
      plot <- sentiment_df %>%
         dplyr::filter( count > n_filter ) %>%  
         # compute normalize value of sentiment 
         mutate( affin_nrm = range01(affin)) %>%
         ggplot( aes(words, count,  color = affin_nrm)) +
            geom_jitter(alpha = 0.2, width=0.2, height = 0.1) + 
            geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) + 
            scale_y_log10() + 
            # split positive - negative 
            facet_wrap(bing~.) +
            theme_minimal() + 
            theme(axis.text.x=element_blank(), 
                  legend.position = "bottom") + 
            labs( color = "Sentiment degree")
      

      
   } else if(ngrams == 3){
            
      sentiment_df <- data %>% 
            inner_join(get_sentiments("afinn"), by=c('word1'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word1'= 'word')) %>%
            inner_join(get_sentiments("afinn"), by=c('word2'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word2'= 'word')) %>%
            inner_join(get_sentiments("afinn"), by=c('word3'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word3'= 'word')) %>%
            mutate( affin = (value.x + value.y + value) / 3) %>%
            mutate( bing = case_when(sentiment.x == 'positive' & 
                                        sentiment.y == 'positive' & 
                                        sentiment == 'positive' ~ 'positive', 
                                     sentiment.x == 'negative' & 
                                        sentiment.y == 'negative' & 
                                        sentiment == 'negative' ~ 'negative',
                                     
                                     sentiment.x == 'positive' & 
                                        sentiment.y == 'positive' & 
                                        sentiment == 'negative' ~ 'neutral-positive',
                                     sentiment.x == 'positive' & 
                                        sentiment.y == 'negative' & 
                                        sentiment == 'positive' ~ 'neutral-positive',
                                     sentiment.x == 'negative' & 
                                        sentiment.y == 'positive' & 
                                        sentiment == 'positive' ~ 'neutral-positive',
                                     
                                     TRUE ~ 'neutral-negative')) %>%
            unite(words, c("word1", "word2", "word3"), sep = " ") 
         
      plot <- sentiment_df %>%
         dplyr::filter( count > n_filter ) %>%  
         mutate( affin_nrm = range01(affin)) %>%
         ggplot( aes(words, count,  color = affin_nrm)) +
            geom_jitter(alpha = 0.2, width=0.2, height = 0.1) + 
            geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) + 
            scale_y_log10() + 
            facet_wrap(bing~.) +
            theme_minimal() + 
            theme(axis.text.x=element_blank(), 
                  legend.position = "bottom") + 
            labs( color = "Sentiment degree")
   } else {
   # plot positive-negative -- color: how much positive/negative is a word
      sentiment_df <- data %>% 
            inner_join(get_sentiments("afinn"), by=c('words'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c("words" = "word"))
      
      plot <- sentiment_df %>%
         dplyr::filter( count > n_filter ) %>%  
            mutate( value_std = range01(value)) %>%
            ggplot( aes(words, count,  color = value_std)) +
               geom_jitter(alpha = 0.2, width=0.2, height = 0.1) +
               geom_text(aes(label = words), check_overlap = TRUE, vjust = 1.5) + 
               facet_wrap(sentiment~.) +
               scale_y_log10() + 
               theme_minimal() + 
               theme(axis.text.x=element_blank(), 
                     legend.position = "bottom") + 
               labs( color = "Sentiment degree")
   }
   print(plot)
   return(sentiment_df)
}



word_network <- function(data, n_filter=1000){
   
   word_graph <- data %>%
      filter(count > n_filter) %>%
      as_tbl_graph()
   
   a <- grid::arrow(type = "open", length = unit(.1, "inches"))
   
   graph <- ggraph(word_graph, layout = "fr") +
        geom_edge_link(aes(edge_alpha = count), show.legend = FALSE,
                       arrow = a, end_cap = circle(.07, 'inches')) +
        geom_node_point(color = "lightblue", size = 1) +
        geom_node_text(aes(label = name), vjust = 1.5, hjust = 1) +
        theme_void()
   
   print(graph)
}

```

Le funzioni definite precedentemente permetto di eseguire un'analisi generale del testo, non legando ciascun testo all'id e autore del commento. Si definisce quindi una funzione che preso il dataset contente i bigrammi e il corpus inziale del dataset, ne esegue il join. 

Per visualizzare come il sentimento 

```{r}
# sentiment over time 

## function to perform the sentiment analysis of each comment 
# data :: tidy_clean_bigrams
# data_txt :: init data corpus
compute_sentiment_per_comment <- function(tidy_clean_bigrams, data_txt){
   
   sentiment_per_doc <- tidy_clean_bigrams  %>% 
            inner_join(get_sentiments("afinn"), by=c('word1'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word1'= 'word')) %>%
            inner_join(get_sentiments("afinn"), by=c('word2'= 'word')) %>%
            inner_join(get_sentiments("bing"), by=c('word2'= 'word')) %>%
            mutate( affin = (value.x + value.y) / 2) %>%
            mutate( bing = case_when(sentiment.x == 'positive' & sentiment.y == 'positive' ~ 'positive', 
                                     sentiment.x == 'negative' & sentiment.y == 'negative' ~ 'negative',
                                     TRUE ~ 'neutral')) %>%
            mutate( affin_nrm = range01(affin)) %>%
            unite(words, c("word1", "word2"), sep = " ") %>%
            mutate(id = as.integer(document)) %>%
            select(id, words, affin_nrm, bing)

   sentiment_per_comment <- sentiment_per_doc %>%
      inner_join(data_txt, by='id') %>%
      select(date, author, id, words, affin_nrm, bing)
   
   return(sentiment_per_comment)
   
}

# Sentiment polarity for an element defines the orientation of the expressed sentiment, i.e., it determines if the text expresses the positive, negative or neutral sentiment of the user about the entity in consideration.

std_nrm <- function(x){
   mean <- mean({{x}})
   std <- sd({{x}})
   norm <- (x - mean)/std
   return(norm)
}

## compute the sentiment polarity for each comment
compute_over_time_polarity_sentiment <- function(data, coef_pos=0.6, coef_neg=0.4){

   over_time_sentiment <- data %>%
            count(bing, date) %>%
            spread(bing, n, fill = 0) %>%
            mutate(polarity = (positive + coef_pos*neutral) - (negative + coef_neg*neutral)) %>%
            mutate(polarity_nrm = std_nrm(polarity))
   return(over_time_sentiment)
         
}

# data :: dataset with polarity for date 
# in_year :: year to visualize 
# cols :: { polarity | polarity_nrm }
plot_year_sentiment<- function(data, in_year, cols){
     plot <- data %>%
         mutate(year = year(date)) %>%
         filter(year == in_year) %>%
         mutate(month = month(date)) %>%
         mutate(dayMonth = as.numeric(day(date))) %>%
        ggplot(aes(dayMonth, {{cols}}, color= I(ifelse( {{cols}} >= 0, 'green3', 'orangered2')))) +
        geom_col() +
        geom_smooth(method = "loess", se = FALSE, aes(color='black')) +
        geom_smooth(method = "lm", se = FALSE, aes(color='steelblue1') ) +
        facet_grid(facets = month ~ ., margins = FALSE)+
        scale_y_continuous(trans = scales::pseudo_log_trans()) +
        theme_minimal() + theme(plot.title = element_text(size = 11),
                                legend.position = 'none')+
        xlab(NULL) + ylab(NULL) +
        ggtitle("Polarity Over Time")
     
     print(plot)
}
```

### DogeCoin's Subreddit

```{r}

if(!(exists('doge_com_raw_data'))){
   doge_com_raw_data <- read.csv("../../../data/dogecoin_com.csv")
}

data <- create_corpus_from_rawdata(doge_com_raw_data)
remove(doge_com_raw_data)

corpus_doge <- data$corpus
saveRDS(corpus_doge, file = "../Data/doge_corpus.rds")

doge_txt <- data$df
write.csv(doge_txt, file="../Data/doge_txt.csv")

remove(data)
head(corpus_doge, 2)
head(doge_txt, 2)

```


```{r doge-unigrams}
doge_tidy <- create_tidy_corpus(corpus_doge)
# todo: hunspell approach to stemming (not porter algorithm)
doge_clean_tidy <- compute_clean_corpus_tidy(doge_tidy, lemma = FALSE, stemming = TRUE)
doge_word_counts <- compute_plot_count_and_freq(doge_clean_tidy)
```

```{r doge-unigrams-sentiment}
plot_class_sentiment(doge_word_counts)
```

```{r doge-word-sentiment}
plot_word_per_sentiment(doge_word_counts, n_filter = 10)
```

```{r doge-word-sentiment}
plot_word_per_sentiment(doge_word_counts, n_filter = 10)
```

```{r}
remove(doge_tidy)
remove(doge_clean_tidy)
remove(doge_word_counts)
```


```{r doge-bigrams}
doge_tidy_bigrams <- create_tidy_corpus(corpus_doge, ngrams=2)
doge_clean_tidy_bigrams <- compute_clean_corpus_tidy(doge_tidy_bigrams, ngrams = 2, lemma = FALSE, stemming = TRUE)
doge_bigrams_counts <- compute_plot_count_and_freq(doge_clean_tidy_bigrams, 
                                                   threshold_freq = 0.0003,
                                                   ngrams = 2)
```

```{r doge-bigrams-sentiment}
bigrams_class_sentiment <- plot_class_sentiment(doge_bigrams_counts, n_filter_sentiment = 2500, ngrams = 2)
```


```{r doge-bigrams-word-sentiment}
bigrams_sentiment <- plot_word_per_sentiment(doge_bigrams_counts, n_filter = 10, ngrams = 2)
```


```{r doge-net-bigrams}
word_network(doge_bigrams_counts)
```

```{r}
remove(doge_tidy_bigrams)
#remove(doge_clean_tidy_bigrams)
remove(doge_bigrams_counts)
```



```{r doge-trigrams, time_it=TRUE}
doge_tidy_trigrams <- create_tidy_corpus(corpus_doge, ngrams=3)
doge_clean_tidy_trigrams <- compute_clean_corpus_tidy(doge_tidy_trigrams, ngrams = 3, lemma = FALSE, stemming = TRUE)
doge_trigrams_counts <- compute_plot_count_and_freq(doge_clean_tidy_trigrams, 
                                                   threshold_freq = 0.00001,
                                                   bool_plot_count = FALSE,
                                                   bool_plot_frequency  = FALSE,
                                                   ngrams = 3)
```

```{r doge-trigrams-sentiment, time_it=TRUE}
plot_class_sentiment(doge_trigrams_counts, n_filter_sentiment = 450, ngrams = 3)
```

```{r doge-trigrams-word-sentiment, time_it=TRUE}
plot_word_per_sentiment(doge_trigrams_counts, n_filter = 2, ngrams = 3)
```

```{r doge-net-trigrams}
word_network(doge_trigrams_counts, n_filter=400)
```

```{r}
remove(doge_tidy_trigrams)
remove(doge_clean_tidy_trigrams)
remove(doge_trigrams_counts)
```


```{r}
doge_per_com_sentiment <- compute_sentiment_per_comment(doge_clean_tidy_bigrams, doge_txt)
doge_polarity <- compute_over_time_polarity_sentiment(doge_per_com_sentiment, coef_pos=0.8, coef_neg=0.2)

plot_year_sentiment(doge_polarity, "2020", polarity)
```

Se andiamo a normalizzare il valore della polarità in modo tale che abbia media zero e standard deviation pari a uno, si può notare con maggior facilità quelli che sono i punti in cui si ha un valore di positività più alto del normale. 

```{r}
plot_year_sentiment(doge_polarity, "2020", polarity_nrm)
```


```{r echo=FALSE}
remove(corpus_doge)
remove(doge_txt)

remove(doge_per_com_sentiment)
remove(doge_polarity)

remove(doge_clean_tidy_bigrams)
remove(bigrams_class_sentiment)
remove(bigrams_sentiment)
```


### Ethereum's Subreddit

```{r}

2
data <- create_corpus_from_rawdata(eth_com_raw_data)
remove(eth_com_raw_data)

corpus_eth <- data$corpus
saveRDS(corpus_eth, file = "../Data/eth_corpus.rds")

eth_txt <- data$df
write.csv(eth_txt, file="../Data/eth_txt.csv")


head(corpus_eth, 2)
head(eth_txt, 2)

```

```{r eth-unigrams}
eth_tidy <- create_tidy_corpus(corpus_eth)
eth_clean_tidy <- compute_clean_corpus_tidy(eth_tidy, lemma = FALSE, stemming = TRUE)
eth_word_counts <- compute_plot_count_and_freq(eth_clean_tidy, threshold_count = 20)

```

```{r eth-unigrams-sentiment}
plot_class_sentiment(eth_word_counts)
```

```{r eth-word-sentiment}
plot_word_per_sentiment(eth_word_counts, n_filter=2)
```

```{r echo=FALSE}
remove(eth_tidy)
remove(eth_clean_tidy)
remove(eth_word_counts)
```


```{r eth-bigrams}
eth_tidy_bigrams <- create_tidy_corpus(corpus_eth, ngrams=2)
eth_clean_tidy_bigrams <- compute_clean_corpus_tidy(eth_tidy_bigrams, ngrams = 2, lemma = FALSE, stemming = TRUE)
eth_bigrams_counts <- compute_plot_count_and_freq(eth_clean_tidy_bigrams, 
                                                   threshold_freq = 0.0005,
                                                   threshold_count = 5, 
                                                   ngrams = 2)
```

```{r eth-bigrams-sentiment}
eth_bigrams_class_sentiment <- plot_class_sentiment(eth_bigrams_counts, n_filter_sentiment = 10, ngrams = 2)
```


```{r eth-bigrams-word-sentiment}
# eth_bigrams_sentiment <- plot_word_per_sentiment(eth_bigrams_counts, n_filter = 0, ngrams = 2)
```


```{r eth-net-bigrams}
word_network(eth_bigrams_counts, n_filter=3)
```

```{r}
remove(eth_tidy_bigrams)
remove(eth_clean_tidy_bigrams)
remove(eth_bigrams_counts)
```



```{r eth-trigrams, time_it=TRUE}
eth_tidy_trigrams <- create_tidy_corpus(corpus_eth, ngrams=3)
eth_clean_tidy_trigrams <- compute_clean_corpus_tidy(eth_tidy_trigrams, ngrams = 3, lemma = FALSE, stemming = TRUE)
eth_trigrams_counts <- compute_plot_count_and_freq(eth_clean_tidy_trigrams, 
                                                   threshold_freq = 0.00001,
                                                   bool_plot_count = FALSE,
                                                   bool_plot_frequency  = FALSE,
                                                   ngrams = 3)
```

```{r eth-trigrams-sentiment, time_it=TRUE}
plot_class_sentiment(eth_trigrams_counts, n_filter_sentiment = 2, ngrams = 3)
```

```{r eth-trigrams-word-sentiment, time_it=TRUE}
# plot_word_per_sentiment(eth_trigrams_counts, n_filter = 0, ngrams = 3)
```

```{r eth-net-trigrams}
word_network(eth_trigrams_counts, n_filter=2)
```

```{r}
remove(eth_tidy_trigrams)
remove(eth_clean_tidy_trigrams)
remove(eth_trigrams_counts)
```


```{r echo=FALSE}
remove(corpus_eth)
remove(eth_txt)

remove(eth_bigrams_class_sentiment)
remove(eth_bigrams_sentiment)
```


### Bitcoin's Subreddit 

```{r}

if(!(exists('btc_com_raw_data'))){
   btc_com_raw_data <- read.csv("../../data/btc_com.csv")
}

data <- create_corpus_from_rawdata(btc_com_raw_data)
remove(btc_com_raw_data)

corpus_btc <- data$corpus
btc_txt <- data$df

head(corpus_btc, 2)
head(btc_txt, 2)

```

```{r btc-unigrams}
btc_tidy <- create_tidy_corpus(corpus_btc)
btc_clean_tidy <- compute_clean_corpus_tidy(btc_tidy, lemma = FALSE, stemming = TRUE)
btc_word_counts <- compute_plot_count_and_freq(btc_clean_tidy, threshold_count = 1000)

```

```{r btc-unigrams-sentiment}
plot_class_sentiment(btc_word_counts)
```

```{r btc-word-sentiment}
plot_word_per_sentiment(btc_word_counts, n_filter=2)
```

```{r echo=FALSE}
remove(btc_tidy)
remove(btc_clean_tidy)
remove(btc_word_counts)
```


```{r btc-bigrams}
btc_tidy_bigrams <- create_tidy_corpus(corpus_btc, ngrams=2)
btc_clean_tidy_bigrams <- compute_clean_corpus_tidy(btc_tidy_bigrams, ngrams = 2, lemma = FALSE, stemming = TRUE)
btc_bigrams_counts <- compute_plot_count_and_freq(btc_clean_tidy_bigrams, 
                                                   threshold_freq = 0.0001,
                                                   threshold_count = 500, 
                                                   ngrams = 2)
```

```{r btc-bigrams-sentiment}
btc_bigrams_class_sentiment <- plot_class_sentiment(btc_bigrams_counts, n_filter_sentiment = 5000, ngrams = 2)
```


```{r btc-bigrams-word-sentiment}
btc_bigrams_sentiment <- plot_word_per_sentiment(btc_bigrams_counts, n_filter = 10, ngrams = 2)
```


```{r btc-net- bigrams}
word_network(btc_bigrams_counts, n_filter=1000)
```

```{r}
remove(btc_tidy_bigrams)
remove(btc_clean_tidy_bigrams)
remove(btc_bigrams_counts)
```



```{r btc-trigrams, time_it=TRUE}
btc_tidy_trigrams <- create_tidy_corpus(corpus_btc, ngrams=3)
btc_clean_tidy_trigrams <- compute_clean_corpus_tidy(btc_tidy_trigrams, ngrams = 3, lemma = FALSE, stemming = TRUE)
btc_trigrams_counts <- compute_plot_count_and_freq(btc_clean_tidy_trigrams, 
                                                   threshold_freq = 0.00001,
                                                   bool_plot_count = FALSE,
                                                   bool_plot_frequency  = FALSE,
                                                   ngrams = 3)
```

```{r btc-trigrams-sentiment, time_it=TRUE}
plot_class_sentiment(btc_trigrams_counts, n_filter_sentiment = 1250, ngrams = 3)
```

```{r btc-trigrams-word-sentiment, time_it=TRUE}
plot_word_per_sentiment(btc_trigrams_counts, n_filter = 3, ngrams = 3)
```

```{r btc-net-trigrams}
#word_network(btc_trigrams_counts, n_filter=10)
```

```{r}
remove(btc_tidy_trigrams)
remove(btc_clean_tidy_trigrams)
remove(btc_trigrams_counts)
```


```{r echo=FALSE}
remove(corpus_btc)
remove(btc_txt)

remove(btc_bigrams_class_sentiment)
remove(btc_bigrams_sentiment)
```


### PancakeSwap's Subreddit

```{r}

if(!(exists('pancake_com_raw_data'))){
   pnc_com_raw_data <- read.csv("../../data/pancakeswap_com.csv")
}

data <- create_corpus_from_rawdata(pnc_com_raw_data)
remove(pnc_com_raw_data)

corpus_pnc <- data$corpus
pnc_txt <- data$df

head(corpus_pnc, 2)
head(pnc_txt, 2)

```

```{r pnc-unigrams}
pnc_tidy <- create_tidy_corpus(corpus_pnc)
pnc_clean_tidy <- compute_clean_corpus_tidy(pnc_tidy, lemma = FALSE, stemming = TRUE)
pnc_word_counts <- compute_plot_count_and_freq(pnc_clean_tidy, threshold_count = 5)

```

```{r pnc-unigrams-sentiment}
plot_class_sentiment(pnc_word_counts)
```

```{r pnc-word-sentiment}
plot_word_per_sentiment(pnc_word_counts, n_filter=0)
```

```{r echo=FALSE}
remove(pnc_tidy)
remove(pnc_clean_tidy)
remove(pnc_word_counts)
```


```{r pnc-bigrams}
pnc_tidy_bigrams <- create_tidy_corpus(corpus_pnc, ngrams=2)
pnc_clean_tidy_bigrams <- compute_clean_corpus_tidy(pnc_tidy_bigrams, ngrams = 2, lemma = FALSE, stemming = TRUE)
pnc_bigrams_counts <- compute_plot_count_and_freq(pnc_clean_tidy_bigrams, 
                                                   threshold_freq = 0.002,
                                                   threshold_count = 2, 
                                                   ngrams = 2)
```

```{r pnc-bigrams-sentiment}
eth_bigrams_class_sentiment <- plot_class_sentiment(pnc_bigrams_counts, n_filter_sentiment = 1, ngrams = 2)
```


```{r pnc-bigrams-word-sentiment}
#pnc_bigrams_sentiment <- plot_word_per_sentiment(pnc_bigrams_counts, n_filter = 0, ngrams = 2)
```


```{r pnc-net-bigrams}
word_network(pnc_bigrams_counts, n_filter=2)
```

```{r}
remove(pnc_tidy_bigrams)
remove(pnc_clean_tidy_bigrams)
remove(pnc_bigrams_counts)
```



```{r pnc-trigrams, time_it=TRUE}
# troppi pochi dati 
pnc_tidy_trigrams <- create_tidy_corpus(corpus_pnc, ngrams=3)
pnc_clean_tidy_trigrams <- compute_clean_corpus_tidy(pnc_tidy_trigrams, ngrams = 3, lemma = FALSE, stemming = TRUE)
pnc_trigrams_counts <- compute_plot_count_and_freq(pnc_clean_tidy_trigrams, 
                                                   bool_plot_count = FALSE,
                                                   bool_plot_frequency  = FALSE,
                                                   ngrams = 3)
```

```{r pnc-trigrams-sentiment, time_it=TRUE}
# plot_class_sentiment(pnc_trigrams_counts, n_filter_sentiment = 0, ngrams = 3)
```

```{r pnc-trigrams-word-sentiment, time_it=TRUE}
# plot_word_per_sentiment(pnc_trigrams_counts, n_filter = 0, ngrams = 3)
```

```{r pnc-net-trigrams}
word_network(pnc_trigrams_counts, n_filter=1)
```

```{r}
remove(pnc_tidy_trigrams)
remove(pnc_clean_tidy_trigrams)
remove(pnc_trigrams_counts)
```


```{r echo=FALSE}
remove(corpus_pnc)
remove(pnc_txt)

remove(pnc_bigrams_class_sentiment)
remove(pnc_bigrams_sentiment)
```

## Network Analysis 


## Results

## Conclusion 

## References



