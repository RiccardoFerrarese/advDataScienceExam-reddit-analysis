---
title: "Analysis"
author: "Riccardo Ferrarese"
date: "9/4/2021"
output: html_document
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

knitr::knit_hooks$set(time_it = local({
  now <- NULL
  function(before, options) {
    if (before) {
      # record the current time before each chunk
      now <<- Sys.time()
    } else {
      # calculate the time difference after a chunk
      res <- difftime(Sys.time(), now)
      # return a character string to show the time
      paste("Time for this code chunk to run:", res)
    }
  }
}))

```


```{r library, echo=FALSE}

# general lib for manipulate data
library(dplyr)
library(tidyr)
library(tidyverse)

# lib for side by side plot 
library(patchwork)

# lib for network analysis
library(ggraph)
library(tidygraph)
library(lubridate)

# lib for text mining
library(tidytext)
library(quanteda)

# lib for sentiment dataset
library(textdata)

```


```{r}

doge_com_raw_data <- read.csv("../data/dogecoin_com.csv")

# eth_sub_raw_data <- read.csv("../ScrapData/data/dogecoin_sub.csv")

# id is for the comment, link_id is the submission's id, and parent_id is the id of the parent of the comment, which can be either another comment or the submission.

doge_sub_raw_data <- read.csv('../data/dogecoin_sub.csv')

```

```{r commet_per_date}

# data must have cols : (created_utc, link_id )
# dataset : source dataset
plot_com_per_date <- function(dataset){

   df_date <- dataset %>%
      select(created_utc, link_id) %>%
      group_by(link_id) %>% 
      mutate(date = as.Dzate(as_datetime(created_utc)))
      
   
   df_date%>%
      group_by(date) %>%
      summarise(n_com = n()) %>%
      arrange(date) %>%
      ggplot(aes(date, n_com)) +
         geom_line(size = 0.3) +
         xlab("Date") + 
         ylab("Number of Comment") + 
         theme_minimal()
}

# not usefull 
plot_sub_per_date <- function(dataset, create_utc, id){
   # dataset must be contain created_utc & link_id
   dataset %>%
      mutate(date = as.Date(as_datetime(created_utc))) %>%
      group_by(date) %>%
      arrange(date) %>%
      ggplot(aes(date, dataset$num_comment)) +
         geom_line(size = 0.3) +
         xlab("Date") + 
         ylab("Submission") + 
         theme_minimal()
}
 
# plot for date of comment 
plot_com_for_date(doge_com_raw_data)

doge_com <- sub_per_date %>%
   filter( date > as.Date('2020-12-31'))

# 2021-01-29	219575			
# 2021-01-30	159410			
# 2021-01-31	102823	
          
```



```{r net_func}

# data must have cols : (created_utc, author, link_id )
# dataf : source dataset
# file_name: name for save network tidy data 
# num_usr: number of top user to extract (bound to 50 usr for computability)
compute_top_usr_net <- function(dataf, file_name, num_usr){
   
   if(num_usr>100){errorCondition("Too much user for compute graph visualization")}
   
   author_link_unique <- dataf %>%
      select(author, link_id) %>%
      filter(author != '[deleted]' & author != 'AutoModerator') %>%
      group_by(link_id) %>%
      unique()
   
   # remove source data for save space in env 
   remove(dataf)
   
   # number of post which each author interact with 
   top_usr <- author_link_unique %>% 
      group_by(author) %>% 
      summarise( n_aut = n()) %>% 
      ungroup() %>% 
      arrange(-n_aut) %>% 
      head(num_usr)
   
   # take usr with most comment
   top_usr_info <- author_link_unique %>%
      filter( author %in% top_usr$author)

   # join for track interaction between most "top" usr 
   net_top_usr <- top_usr_info %>%
      inner_join(top_usr_info, by='link_id') %>%
      filter(author.x != author.y) %>%
      select(author.x, author.y, link_id)
   
   # write it on file 
   write.csv(net_top_usr, file=file_name,
          row.names=FALSE)
   
   return(top_usr_info)
}

doge_com_raw_data <- read.csv("../data/dogecoin_com.csv")
top_usr_info <- compute_top_usr_net(doge_com_raw_data, "./data/usr_doge_net50", 50)

```

```{r net_doge}
usr_net <- read.csv('data/usr_doge_net50')

usr_net %>% rename(
   from = author.x, 
   to = author.y, 
   post_id = link_id
)

graph_topcom_doge <- usr_net %>% 
   as_tbl_graph()

graph_topcom_doge %>% 
   activate(nodes) %>%
   # compute centrality 
   mutate(eigenv = centrality_eigen()) %>%
   activate(edges) %>%
   mutate(betweenness = centrality_edge_betweenness()) %>%
   ggraph(layout='kk') +
   geom_edge_link(aes(colour = link_id),  show.legend = FALSE) + # width=betweenness
   geom_node_point(aes(size = eigenv, alpha = eigenv), colour = 'black' ) + 
   geom_node_text(aes(label = name),  repel = TRUE)

## non calcolo il pagerank essendo un grafo indiretto 
# 
   
```
```{r}

# in questo caso la betweeness potrebbe essere intesa come l'influeza che un nodo ha avendo interagito con 
# diversi commenti, e quindi molto attivo nella comunità interagendo con molte persone diverse
# e quindi si trova su più shortest path 

graph_topcom_doge %>% 
   activate(nodes) %>%
   # compute influence
   mutate(betweenness = centrality_betweenness()) %>%
   ggraph(layout='kk') +
   geom_edge_link(aes( colour = link_id), size=0.1, show.legend = FALSE) +
   geom_node_point(aes(size = betweenness, alpha = betweenness) )+ 
   geom_node_text(aes(label = name),  repel = TRUE)

```


```{r doge-unigrams}
doge_tidy <- create_tidy_corpus(corpus_doge)
doge_clean_tidy <- clean_corpus_tidy(doge_tidy)
doge_word_counts <- compute_plot_count_and_freq(doge_clean_tidy)
```

```{r doge-unigrams-sentiment}
plot_class_sentiment(doge_word_counts)
```

```{r doge-word-sentiment}
plot_word_per_sentiment(doge_word_counts, n_filter = 10)
```

```{r}
remove(doge_tidy)
remove(doge_clean_tidy)
remove(doge_word_counts)
```


```{r doge-bigrams}
doge_tidy_bigrams <- create_tidy_corpus(corpus_doge, ngrams=2)
doge_clean_tidy_bigrams <- clean_corpus_tidy(doge_tidy_bigrams, ngrams = 2)
doge_bigrams_counts <- compute_plot_count_and_freq(doge_clean_tidy_bigrams, 
                                                   threshold_freq = 0.0003,
                                                   ngrams = 2)
```

```{r doge-bigrams-sentiment}
bigrams_class_sentiment <- plot_class_sentiment(doge_bigrams_counts, n_filter_sentiment = 2500, ngrams = 2)
```


```{r doge-bigrams-word-sentiment}
bigrams_sentiment <- plot_word_per_sentiment(doge_bigrams_counts, n_filter = 10, ngrams = 2)
```


```{r net-bigrams}
word_network(doge_bigrams_counts)
```

```{r}
remove(doge_tidy_bigrams)
remove(doge_clean_tidy_bigrams)
remove(doge_bigrams_counts)
```



```{r doge-trigrams, time_it=TRUE}
doge_tidy_trigrams <- create_tidy_corpus(corpus_doge, ngrams=3)
doge_clean_tidy_trigrams <- clean_corpus_tidy(doge_tidy_trigrams, ngrams = 3)
doge_trigrams_counts <- compute_plot_count_and_freq(doge_clean_tidy_trigrams, 
                                                   threshold_freq = 0.00001,
                                                   bool_plot_count = FALSE,
                                                   bool_plot_frequency  = FALSE,
                                                   ngrams = 3)
```

```{r doge-trigrams-sentiment, time_it=TRUE}
plot_class_sentiment(doge_trigrams_counts, n_filter_sentiment = 450, ngrams = 3)
```

```{r doge-trigrams-word-sentiment, time_it=TRUE}
plot_word_per_sentiment(doge_trigrams_counts, n_filter = 2, ngrams = 3)
```

```{r net-trigrams}
word_network(doge_trigrams_counts, n_filter=400)
```

```{r}
remove(doge_tidy_trigrams)
remove(doge_clean_tidy_trigrams)
remove(doge_trigrams_counts)
```




